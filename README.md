# Asistente MatemÃ¡tico con LLMs Locales

Este proyecto implementa un sistema para evaluar modelos de lenguaje (LLMs) locales en la resoluciÃ³n de problemas matemÃ¡ticos. Utilizando [Ollama](https://ollama.ai/) como motor de ejecuciÃ³n, permite comparar diferentes modelos como Llama 2 y Llama 3.2 en su capacidad para resolver problemas matemÃ¡ticos.

![Vista previa](https://github.com/user/repo/assets/preview.png)

## CaracterÃ­sticas principales

- ğŸ§® **ResoluciÃ³n de problemas matemÃ¡ticos** usando LLMs locales
- ğŸ“Š **Visualizaciones comparativas** de rendimiento entre modelos
- ğŸ” **AnÃ¡lisis detallado** de tiempos de respuesta y precisiÃ³n
- ğŸ“± **Interfaz web** para fÃ¡cil interacciÃ³n con los modelos
- ğŸ“ˆ **GeneraciÃ³n de reportes** para anÃ¡lisis cualitativos

## InstalaciÃ³n rÃ¡pida

### Prerrequisitos

- Python 3.8+
- [Ollama](https://ollama.ai/) instalado con modelos llama2 y llama3.2

### Pasos de instalaciÃ³n

1. Clonar el repositorio:
   ```bash
   git clone https://github.com/user/proyecto-llm-matematicas.git
   cd proyecto-llm-matematicas
   ```

2. Crear y activar entorno virtual:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Linux/macOS
   # o
   .venv\Scripts\activate     # Windows
   ```

3. Instalar dependencias:
   ```bash
   cd api
   pip install -r requirements.txt
   ```

## Uso

### Iniciar el servidor API

```bash
cd api
source .venv/bin/activate  # Si no estÃ¡ activado
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Iniciar la interfaz web

En una nueva terminal:

```bash
cd api
source .venv/bin/activate
streamlit run ui.py
```

La interfaz web se abrirÃ¡ automÃ¡ticamente en `http://localhost:8501`.

## Modos de uso

### Modo Individual

- Selecciona un modelo
- Escribe un problema matemÃ¡tico
- ObtÃ©n la respuesta al instante

### Modo Test

- Selecciona mÃºltiples modelos para comparar
- Elige entre 10 problemas matemÃ¡ticos predefinidos
- Visualiza resultados con grÃ¡ficos comparativos:
  - Barras horizontales por problema
  - Diagrama de Gantt de tiempos
  - GrÃ¡fico de radar para patrones de rendimiento
  - Comparativa directa entre modelos

## GeneraciÃ³n de reportes

DespuÃ©s de ejecutar pruebas, puedes generar un reporte detallado:

1. Descarga los resultados JSON desde la UI
2. Ejecuta el generador de reportes:
   ```bash
   cd api
   python generate_report.py ruta/a/resultados_test.json -o ../docs/mi_evaluacion.md
   ```

El reporte incluirÃ¡:
- InformaciÃ³n del sistema (CPU, RAM, GPU)
- Comparativa de tiempos entre modelos
- Plantilla para evaluar la calidad de las respuestas

## Problemas matemÃ¡ticos incluidos

El sistema incluye 10 problemas matemÃ¡ticos que cubren:

1. Probabilidad bÃ¡sica (suma de dados)
2. CÃ¡lculo - integrales
3. TeorÃ­a de nÃºmeros (conteo de primos)
4. Ecuaciones diferenciales
5. Problemas de fÃ­sica - cinemÃ¡tica
6. Desarrollo de expresiones algebraicas
7. Probabilidad avanzada
8. Sistemas de ecuaciones lineales
9. CÃ¡lculo - derivadas
10. GeometrÃ­a (triÃ¡ngulos)

## Estructura del proyecto

```
proyecto-llm-matematicas/
â”œâ”€â”€ api/                # Servidor FastAPI y UI de Streamlit
â”‚   â”œâ”€â”€ main.py         # API REST para interactuar con Ollama
â”‚   â”œâ”€â”€ ui.py           # Interfaz de usuario con Streamlit
â”‚   â”œâ”€â”€ generate_report.py # Generador de reportes de evaluaciÃ³n
â”‚   â””â”€â”€ requirements.txt # Dependencias del proyecto
â”œâ”€â”€ prompts/            # Problemas matemÃ¡ticos predefinidos
â”‚   â”œâ”€â”€ benchmark_ej1.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ outputs/            # Almacenamiento de resultados
â”‚   â””â”€â”€ benchmark/      # Resultados de comparativas
â”œâ”€â”€ docs/               # DocumentaciÃ³n y reportes
â””â”€â”€ README.md           # Este archivo
```

## Contribuciones

Las contribuciones son bienvenidas. Puedes:
- AÃ±adir nuevos problemas matemÃ¡ticos en `prompts/`
- Mejorar las visualizaciones en `api/ui.py`
- AÃ±adir soporte para nuevos modelos

## Licencia

Este proyecto estÃ¡ disponible bajo la licencia MIT.
